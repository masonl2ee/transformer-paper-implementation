{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# <p style= \"font-weight: bold; font-size: 40px;\">\"KO-EN Translator\" by implementation of Transformer model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style= \"font-weight: bold\">1. Library & GPU setting(Mac)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from konlpy.tag import Mecab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter # used for building vocabulary\n",
    "from einops import rearrange, reduce, repeat\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "print(\"MPS is available:\", torch.backends.mps.is_available())\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "FP16 = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style= \"font-weight: bold\">2. DataRead&Tokenizing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "  with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "  content = [line.strip() for line in content]\n",
    "  return content\n",
    "\n",
    "train_src = read_file('./data/korean-english-park.train/korean-english-park.train.ko')\n",
    "train_trg = read_file('./data/korean-english-park.train/korean-english-park.train.en')\n",
    "valid_src = read_file('./data/korean-english-park.dev/korean-english-park.dev.ko')\n",
    "valid_trg = read_file('./data/korean-english-park.dev/korean-english-park.dev.en')\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def tokenizer_ko(ko_sentences):\n",
    "  ko_sentences = [mecab.morphs(sentence) for sentence in ko_sentences]\n",
    "  return ko_sentences\n",
    "\n",
    "def tokenizer_en(en_sentences):\n",
    "  en_sentences = [nltk.word_tokenize(sentence) for sentence in en_sentences]\n",
    "  return en_sentences\n",
    "\n",
    "train_src = tokenizer_ko(train_src)\n",
    "train_trg = tokenizer_en(train_trg)\n",
    "valid_src = tokenizer_ko(valid_src)\n",
    "valid_trg = tokenizer_en(valid_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['개인', '용', '컴퓨터', '사용', '의', '상당', '부분', '은', '\"', '이것', '보다', '뛰어날', '수', '있', '느냐', '?', '\"'], ['모든', '광', '마우스', '와', '마찬가지', '로', '이', '광', '마우스', '도', '책상', '위', '에', '놓', '는', '마우스', '패드', '를', '필요', '로', '하', '지', '않', '는다', '.'], ['그러나', '이것', '은', '또한', '책상', '도', '필요', '로', '하', '지', '않', '는다', '.'], ['79', '.', '95', '달러', '하', '는', '이', '최첨단', '무선', '광', '마우스', '는', '허공', '에서', '팔목', ',', '팔', ',', '그', '외', '에', '어떤', '부분', '이', '든', '그', '움직임', '에', '따라', '커서', '의', '움직임', '을', '조절', '하', '는', '회전', '운동', '센서', '를', '사용', '하', '고', '있', '다', '.'], ['정보', '관리', '들', '은', '동남', '아시아', '에서', '의', '선박', '들', '에', '대한', '많', '은', '(', '테러', ')', '계획', '들', '이', '실패', '로', '돌아갔', '음', '을', '밝혔으며', ',', '세계', '해상', '교역', '량', '의', '거의', '3', '분', '의', '1', '을', '운송', '하', '는', '좁', '은', '해', '로', '인', '말라카', '해협', '이', '테러', '공격', '을', '당하', '기', '쉽', '다고', '경고', '하', '고', '있', '다', '.']]\n",
      "#######################\n",
      "[['Much', 'of', 'personal', 'computing', 'is', 'about', '``', 'can', 'you', 'top', 'this', '?', \"''\"], ['so', 'a', 'mention', 'a', 'few', 'weeks', 'ago', 'about', 'a', 'rechargeable', 'wireless', 'optical', 'mouse', 'brought', 'in', 'another', 'rechargeable', ',', 'wireless', 'mouse', '.'], ['Like', 'all', 'optical', 'mice', ',', 'But', 'it', 'also', 'does', \"n't\", 'need', 'a', 'desk', '.'], ['uses', 'gyroscopic', 'sensors', 'to', 'control', 'the', 'cursor', 'movement', 'as', 'you', 'move', 'your', 'wrist', ',', 'arm', ',', 'whatever', 'through', 'the', 'air', '.'], ['Intelligence', 'officials', 'have', 'revealed', 'a', 'spate', 'of', 'foiled', 'plots', 'on', 'ships', 'in', 'Southeast', 'Asia', 'and', 'are', 'warning', 'that', 'a', 'narrow', 'stretch', 'of', 'water', 'carrying', 'almost', 'one', 'third', 'of', 'the', 'world', \"'s\", 'maritime', 'trade', 'is', 'vulnerable', 'to', 'a', 'terror', 'attack', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(train_src[0: 5])\n",
    "print(\"#######################\")\n",
    "print(train_trg[0: 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style= \"font-weight: bold\">3. Build vacabulary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_sentences, max_size = 50000, min_freq = 2):\n",
    "  word_counts = Counter()\n",
    "\n",
    "  for sentence in tokenized_sentences:\n",
    "    word_counts.update(sentence)\n",
    "\n",
    "  special_token = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "  vocab = {token: idx for idx, token in enumerate(special_token)}\n",
    "\n",
    "  words = [word for word, count in word_counts.most_common(max_size - len(special_token)) if count >= min_freq]\n",
    "\n",
    "  for word in words:\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "  idx2word = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "  return vocab, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src, idx2word_src = build_vocab(train_src)\n",
    "vocab_trg, idx2word_trg = build_vocab(train_trg)\n",
    "vocab_src_valid, idx2word_src_valid = build_vocab(valid_src)\n",
    "vocab_trg_valid, idx2word_trg_valid = build_vocab(valid_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "N = 2\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 64\n",
    "INNER_DIM = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_HEAD = 4 \n",
    "WEIGHT_DECAY = 0\n",
    "\n",
    "\n",
    "SEQ_LEN = 64\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, src_data, trg_data, src_vocab, trg_vocab):\n",
    "    super().__init__()\n",
    "    self.src_data = src_data\n",
    "    self.trg_data = trg_data\n",
    "    self.src_vocab = src_vocab\n",
    "    self.trg_vocab = trg_vocab\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.src_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    src = [SOS_TOKEN] + self.src_data[idx] + [EOS_TOKEN]\n",
    "    trg_input = [SOS_TOKEN] + self.trg_data[idx]\n",
    "    trg_output = self.trg_data[idx] + [EOS_TOKEN]\n",
    "    \n",
    "    # 토큰을 인덱스로 변환\n",
    "    src_indices = [self.src_vocab.get(token, self.src_vocab[UNK_TOKEN]) for token in src]\n",
    "    trg_input_indices = [self.trg_vocab.get(token, self.trg_vocab[UNK_TOKEN]) for token in trg_input]\n",
    "    trg_output_indices = [self.trg_vocab.get(token, self.trg_vocab[UNK_TOKEN]) for token in trg_output]\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "    trg_input_tensor = torch.tensor(trg_input_indices, dtype=torch.long)\n",
    "    trg_output_tensor = torch.tensor(trg_output_indices, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        'src': src_tensor,\n",
    "        'trg_input': trg_input_tensor,\n",
    "        'trg_output': trg_output_tensor\n",
    "    }\n",
    "  \n",
    "class ValidDataset(Dataset):\n",
    "  def __init__(self, src_data, trg_data, src_vocab, trg_vocab):\n",
    "    super().__init__()\n",
    "    self.src_data = src_data\n",
    "    self.trg_data = trg_data\n",
    "    self.src_vocab = src_vocab\n",
    "    self.trg_vocab = trg_vocab\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.src_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    src = [SOS_TOKEN] + self.src_data[idx] + [EOS_TOKEN]\n",
    "    trg_input = [SOS_TOKEN] + self.trg_data[idx]\n",
    "    trg_output = self.trg_data[idx] + [EOS_TOKEN]\n",
    "    \n",
    "    # 토큰을 인덱스로 변환\n",
    "    src_indices = [self.src_vocab.get(token, self.src_vocab[UNK_TOKEN]) for token in src]\n",
    "    trg_input_indices = [self.trg_vocab.get(token, self.trg_vocab[UNK_TOKEN]) for token in trg_input]\n",
    "    trg_output_indices = [self.trg_vocab.get(token, self.trg_vocab[UNK_TOKEN]) for token in trg_output]\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "    trg_input_tensor = torch.tensor(trg_input_indices, dtype=torch.long)\n",
    "    trg_output_tensor = torch.tensor(trg_output_indices, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        'src': src_tensor,\n",
    "        'trg_input': trg_input_tensor,\n",
    "        'trg_output': trg_output_tensor\n",
    "    }    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  src_tensors = [item['src'] for item in batch]\n",
    "  trg_input_tensors = [item['trg_input'] for item in batch]\n",
    "  trg_output_tensors = [item['trg_output'] for item in batch]\n",
    "\n",
    "  src_padded = pad_sequence(src_tensors, batch_first=True, padding_value=vocab_src[PAD_TOKEN])\n",
    "  trg_input_padded = pad_sequence(trg_input_tensors, batch_first=True, padding_value=vocab_trg[PAD_TOKEN])\n",
    "  trg_output_padded = pad_sequence(trg_output_tensors, batch_first=True, padding_value=vocab_trg[PAD_TOKEN])\n",
    "\n",
    "  return {\n",
    "    'src': src_padded,\n",
    "    'trg_input': trg_input_padded,\n",
    "    'trg_output': trg_output_padded\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터로더 정의\n",
    "train_dataset = TrainDataset(train_src, train_trg, vocab_src, vocab_trg)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn = collate_fn,\n",
    "    shuffle=True, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_dataset = ValidDataset(valid_src, valid_trg, vocab_src_valid, vocab_trg_valid)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn = collate_fn,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 타입: <class 'dict'>\n",
      "배치 키: dict_keys(['src', 'trg_input', 'trg_output'])\n",
      "\n",
      "소스 텐서 형태: torch.Size([32, 61])\n",
      "소스 텐서 예시 (첫 번째 문장): tensor([    1, 34183,  9572, 15519, 10159,   375,  8574, 19174,  6795,  4578,\n",
      "         2611,    10,  8271,    21,  2709,  6071,    19, 17608,  2163,  3183,\n",
      "           10,  7279,   408,     4,   188,     2,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0])\n",
      "소스 입력: ['<sos>', '미시시피', '외', '에', '앨라배마', '와', '빌', '클린턴', '전', '대통령', '이', '주지사', '를', '지낸', '아칸소', '도', '인종', '에', '따라', '선호', '하', '는', '후보', '가', '극명', '하', '게', '갈렸', '다', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "타겟 입력 텐서 형태: torch.Size([32, 54])\n",
      "타겟 입력 텐서 예시 (첫 번째 문장): tensor([    1,   785,     7,     4,   563,   402,     5,   139,    58,    66,\n",
      "         1028,   767,    36,    23, 14687, 25818,  2080,  4729,     5,    11,\n",
      "          217,    12,   155,   182,   162,     8,  6458,     6,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "\n",
      "타겟 출력 텐서 형태: torch.Size([32, 54])\n",
      "타겟 출력 텐서 예시 (첫 번째 문장): tensor([  785,     7,     4,   563,   402,     5,   139,    58,    66,  1028,\n",
      "          767,    36,    23, 14687, 25818,  2080,  4729,     5,    11,   217,\n",
      "           12,   155,   182,   162,     8,  6458,     6,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "\n",
      "첫 번째 문장 해석:\n",
      "소스: ['<sos>', '미시시피', '외', '에', '앨라배마', '와', '빌', '클린턴', '전', '대통령', '이', '주지사', '를', '지낸', '아칸소', '도', '인종', '에', '따라', '선호', '하', '는', '후보', '가', '극명', '하', '게', '갈렸', '다', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "타겟 입력: ['<sos>', 'According', 'to', 'the', 'Associated', 'Press', ',', 'only', 'two', 'other', 'primary', 'states', 'were', 'as', 'racially', 'polarized', 'neighboring', 'Alabama', ',', 'and', 'Clinton', \"'s\", 'former', 'home', 'state', 'of', 'Arkansas', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "타겟 출력: ['According', 'to', 'the', 'Associated', 'Press', ',', 'only', 'two', 'other', 'primary', 'states', 'were', 'as', 'racially', 'polarized', 'neighboring', 'Alabama', ',', 'and', 'Clinton', \"'s\", 'former', 'home', 'state', 'of', 'Arkansas', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "패딩 비율: 46.62%\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 배치 가져와서 확인하기\n",
    "for batch in train_dataloader:\n",
    "    print(\"배치 타입:\", type(batch))\n",
    "    print(\"배치 키:\", batch.keys())\n",
    "    \n",
    "    # 소스 문장 확인\n",
    "    print(\"\\n소스 텐서 형태:\", batch['src'].shape)\n",
    "    print(\"소스 텐서 예시 (첫 번째 문장):\", batch['src'][1])\n",
    "    print(\"소스 입력:\", [idx2word_src.get(idx.item(), \"<unk>\") for idx in batch['src'][0]])\n",
    "    \n",
    "    # 타겟 입력 확인\n",
    "    print(\"\\n타겟 입력 텐서 형태:\", batch['trg_input'].shape)\n",
    "    print(\"타겟 입력 텐서 예시 (첫 번째 문장):\", batch['trg_input'][0])\n",
    "    \n",
    "    # 타겟 출력 확인\n",
    "    print(\"\\n타겟 출력 텐서 형태:\", batch['trg_output'].shape) \n",
    "    print(\"타겟 출력 텐서 예시 (첫 번째 문장):\", batch['trg_output'][0])\n",
    "    \n",
    "    # 실제 토큰으로 변환 (선택적)\n",
    "    print(\"\\n첫 번째 문장 해석:\")\n",
    "    print(\"소스:\", [idx2word_src.get(idx.item(), \"<unk>\") for idx in batch['src'][0]])\n",
    "    print(\"타겟 입력:\", [idx2word_trg.get(idx.item(), \"<unk>\") for idx in batch['trg_input'][0]])\n",
    "    print(\"타겟 출력:\", [idx2word_trg.get(idx.item(), \"<unk>\") for idx in batch['trg_output'][0]])\n",
    "    \n",
    "    # 패딩 비율 계산\n",
    "    src_pad_count = (batch['src'] == vocab_src[PAD_TOKEN]).sum().item()\n",
    "    src_total = batch['src'].numel()\n",
    "    print(f\"\\n패딩 비율: {src_pad_count/src_total:.2%}\")\n",
    "    \n",
    "    # 한 배치만 확인하고 종료\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodeing(nn.Module):\n",
    "  def __init__(self, hidden_dim, max_len = 5000):\n",
    "    super().__init__()\n",
    "\n",
    "    pe = torch.zeros(max_len, hidden_dim)\n",
    "    pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    _2i = torch.arange(0, hidden_dim, step=2, dtype=torch.float)\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(pos/(10000**(_2i/hidden_dim)))\n",
    "    pe[:, 1::2] = torch.cos(pos/(10000**(_2i/hidden_dim)))\n",
    "\n",
    "    pe = pe.unsqueeze(0)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMask(tensor: torch.Tensor, option: str):\n",
    "    # tensor -> bs, seq_len\n",
    "    if option == 'padding':\n",
    "        # 패딩 토큰(PAD_TOKEN)이 있는 위치에 1, 그 외에는 0\n",
    "        mask = (tensor == vocab_src[PAD_TOKEN]).to(device).unsqueeze(1).unsqueeze(2)\n",
    "        # print('마스크 함수 :', mask.size())\n",
    "        \n",
    "    elif option == 'look_ahead':\n",
    "        # 패딩 마스크: 패딩 토큰이 있는 위치에 1\n",
    "        padding_mask = (tensor == vocab_src[PAD_TOKEN]).to(device).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 룩어헤드 마스크: 미래 토큰이 있는 위치에 1 (대각선 위)\n",
    "        look_ahead_mask = torch.triu(torch.ones(tensor.size(1), tensor.size(1)), diagonal=1).to(device).bool()\n",
    "        look_ahead_mask = look_ahead_mask.to(device).unsqueeze(0).unsqueeze(1)\n",
    "        \n",
    "        # 두 마스크 결합 (OR 연산): 패딩이거나 미래 토큰이면 1\n",
    "        mask = padding_mask | look_ahead_mask\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tensor:\n",
      "tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 0]])\n",
      "\n",
      "Padding mask:\n",
      "tensor([[[[False, False, False, False,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False,  True,  True]]]], device='mps:0')\n",
      "torch.Size([2, 1, 1, 5])\n",
      "torch.Size([2, 1, 5, 5])\n",
      "\n",
      "Look-ahead mask (combined with padding):\n",
      "Sample 0:\n",
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]], device='mps:0')\n",
      "Sample 1:\n",
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def test_mask_function():\n",
    "    # 테스트용 텐서 생성\n",
    "    batch_size = 2\n",
    "    num_head = 2\n",
    "    seq_len = 5\n",
    "    pad_idx = vocab_src[PAD_TOKEN]  # 실제 값으로 대체 필요\n",
    "    \n",
    "    # 샘플 시퀀스 생성 (마지막 위치에 패딩 포함)\n",
    "    test_tensor = torch.ones(batch_size, seq_len, dtype=torch.long)\n",
    "    test_tensor[0, -1] = pad_idx  # 첫 번째 샘플의 마지막 토큰을 패딩으로 설정\n",
    "    test_tensor[1, -2:] = pad_idx  # 두 번째 샘플의 마지막 두 토큰을 패딩으로 설정\n",
    "    \n",
    "    print(\"Test tensor:\")\n",
    "    print(test_tensor)\n",
    "    \n",
    "    # 패딩 마스크 테스트\n",
    "    padding_mask = makeMask(test_tensor, 'padding')\n",
    "    print(\"\\nPadding mask:\")\n",
    "    print(padding_mask)\n",
    "    print(padding_mask.shape)\n",
    "    \n",
    "    # Look-ahead 마스크 테스트\n",
    "    look_ahead_mask = makeMask(test_tensor, 'look_ahead')\n",
    "    print(look_ahead_mask.shape)\n",
    "    print(\"\\nLook-ahead mask (combined with padding):\")\n",
    "    for i in range(batch_size):\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(look_ahead_mask[i])\n",
    "\n",
    "# 테스트 실행\n",
    "test_mask_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_head, hidden_dim, device, dropout = 0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    assert hidden_dim % num_head == 0\n",
    "\n",
    "\n",
    "    self.num_head = num_head\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.head_dim = hidden_dim//num_head\n",
    "    \n",
    "    self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float)).to(device)\n",
    "\n",
    "\n",
    "  def forward(self, query, key, value, mask = None):\n",
    "    batch_size = query.size(0)\n",
    "    Q = self.fc_q(query)\n",
    "    K = self.fc_k(key)\n",
    "    V = self.fc_v(value)\n",
    "\n",
    "    # (bs, seq_len, hidden_dim)\n",
    "\n",
    "    Q = Q.view(batch_size, int(self.num_head), -1, int(self.head_dim))\n",
    "    K = K.view(batch_size, int(self.num_head), -1, int(self.head_dim))\n",
    "    V = V.view(batch_size, int(self.num_head), -1, int(self.head_dim))\n",
    "\n",
    "\n",
    "    # (bs, num_head, seq_len, head_dim)\n",
    "\n",
    "    score = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "    if mask is not None:\n",
    "      # print('마스크 size', mask.size())\n",
    "      score = score.masked_fill(mask == 1, -1e10)\n",
    "\n",
    "    score = torch.softmax(score, dim = -1)\n",
    "\n",
    "    x = torch.matmul(self.dropout(score), V)\n",
    "    \n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    output = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "    output = self.fc_o(output)\n",
    "\n",
    "    return output, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2\n",
    "y = 2\n",
    "x//y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "  def __init__(self, hidden_dim, inner_dim, dropout = 0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear1 = nn.Linear(hidden_dim, inner_dim)\n",
    "    self.linear2 = nn.Linear(inner_dim, hidden_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.linear1(x))\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear2(x)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, hidden_dim, num_head, inner_dim, device, dropout = 0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multiHeadAttention = MultiHeadAttention(num_head, hidden_dim, device)\n",
    "    self.layerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "    self.ffn = FeedForwardLayer(hidden_dim, inner_dim)\n",
    "    self.layerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, input, mask=None):\n",
    "    x, attention = self.multiHeadAttention(query=input, key=input, value=input, mask=mask)\n",
    "    x = self.dropout1(x)\n",
    "    x = input + x\n",
    "    output = self.layerNorm1(x)\n",
    "    output = self.ffn(output)\n",
    "    output = self.dropout1(output)\n",
    "    output = input + output\n",
    "    output = self.layerNorm2(output)\n",
    "\n",
    "    # bs seq_len hidden_dim\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, N, hidden_dim, num_head, inner_dim, device):\n",
    "    super().__init__()\n",
    "    vocab_size = len(vocab_src)\n",
    "    self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
    "    self.positionalEncoding = PositionalEncodeing(hidden_dim)\n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_head, inner_dim, device) for _ in range(N)])\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    embeded_input = self.embedding(input)*self.scale\n",
    "    encoder_input = self.dropout(self.positionalEncoding(embeded_input))\n",
    "    mask = makeMask(input, option='padding')\n",
    "    for enc_layer in self.encoder_layers:\n",
    "      output = enc_layer(encoder_input, mask)\n",
    "      encoder_input = output\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayers(nn.Module):\n",
    "  def __init__(self, hidden_dim, num_head, inner_dim, device, dropout = 0.1):\n",
    "    super().__init__()\n",
    "    self.multiHeadAttention1 = MultiHeadAttention(num_head, hidden_dim, device)\n",
    "    self.LayerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "    self.multiHeadAttention2 = MultiHeadAttention(num_head, hidden_dim, device)\n",
    "    self.LayerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "    self.ffn = FeedForwardLayer(hidden_dim, inner_dim)\n",
    "    self.LayerNorm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, input, encoder_output, paddingMask, lookaheadMask):\n",
    "    x, attention = self.multiHeadAttention1(query=input, key=input, value=input, mask=lookaheadMask)\n",
    "    x = self.dropout1(x)\n",
    "    x = input + x\n",
    "    x = self.LayerNorm1(x)\n",
    "\n",
    "    output_, attention = self.multiHeadAttention2(query=x, key=encoder_output, value=encoder_output, mask=paddingMask)\n",
    "    output_ = self.dropout1(output_)\n",
    "    output_ = x + output_\n",
    "    output_ = self.LayerNorm2(output_)\n",
    "\n",
    "    output = self.ffn(output_)\n",
    "    output = self.dropout1(output)\n",
    "    output = output_ + output\n",
    "    output = self.LayerNorm3(output)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, N, hidden_dim, num_head, inner_dim, device):\n",
    "    super().__init__()\n",
    "    vocab_size = len(vocab_trg)\n",
    "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "    self.positionalEmbedding = PositionalEncodeing(hidden_dim)\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayers(hidden_dim, num_head, inner_dim, device) for _ in range(N)])\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "    self.finalFc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "  def forward(self, input, enc_src, encoder_output):\n",
    "    embedding_input = self.embedding(input) * self.scale\n",
    "    decoder_input = self.positionalEmbedding(embedding_input)\n",
    "    decoder_input = self.dropout(decoder_input)\n",
    "    lookaheadMask = makeMask(input, option='look_ahead')\n",
    "    paddingMask = makeMask(enc_src, option='padding')\n",
    "    for decoder_layer in self.decoder_layers:\n",
    "      output = decoder_layer(decoder_input, encoder_output, paddingMask, lookaheadMask)\n",
    "      decoder_input = output\n",
    "\n",
    "    logits = self.finalFc(output)\n",
    "    output = torch.softmax(logits, dim = -1)\n",
    "    output = torch.argmax(output, dim = -1)\n",
    "    return logits, output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, N, hidden_dim, num_head, inner_dim, device):\n",
    "    super().__init__()\n",
    "    vocab_size = len(valid_trg)\n",
    "\n",
    "    self.encoder = Encoder(N, hidden_dim, num_head, inner_dim, device)\n",
    "    self.decoder = Decoder(N, hidden_dim, num_head, inner_dim, device)\n",
    "\n",
    "    self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "  def forward(self, enc_src, dec_src):\n",
    "    # print('enc_src:', enc_src.size())\n",
    "    # print('dec_src:', dec_src.size())\n",
    "    encoder_output = self.encoder(enc_src)\n",
    "    # print('enc_output:', encoder_output.size())\n",
    "    logits, output = self.decoder(dec_src, enc_src, encoder_output)\n",
    "    # print('dec_output:', output.size())\n",
    "\n",
    "    # print('output size:', output.size())\n",
    "\n",
    "    return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(N, HIDDEN_DIM, NUM_HEAD, INNER_DIM, device).to(device)\n",
    "ic.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "Transformer                                   --\n",
       "├─Encoder: 1-1                                --\n",
       "│    └─Embedding: 2-1                         2,280,256\n",
       "│    └─PositionalEncodeing: 2-2               --\n",
       "│    └─ModuleList: 2-3                        --\n",
       "│    │    └─EncoderLayer: 3-1                 33,472\n",
       "│    │    └─EncoderLayer: 3-2                 33,472\n",
       "│    └─Dropout: 2-4                           --\n",
       "├─Decoder: 1-2                                --\n",
       "│    └─Embedding: 2-5                         2,522,304\n",
       "│    └─PositionalEncodeing: 2-6               --\n",
       "│    └─ModuleList: 2-7                        --\n",
       "│    │    └─DecoderLayers: 3-3                50,240\n",
       "│    │    └─DecoderLayers: 3-4                50,240\n",
       "│    └─Dropout: 2-8                           --\n",
       "│    └─Linear: 2-9                            2,561,715\n",
       "├─Linear: 1-3                                 65,000\n",
       "======================================================================\n",
       "Total params: 7,596,699\n",
       "Trainable params: 7,596,699\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    src = batch['src']\n",
    "    trg_input = batch['trg_input']\n",
    "    break\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, \n",
    "        input_data1=src, input_data2=trg_input,  # 두 개의 입력 텐서 전달\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.named_parameters():\n",
    "    if 'weight' in param[0] and 'layerNorm' not in param[0]:\n",
    "        # 텐서가 최소 2차원 이상인지 확인\n",
    "        if param[1].dim() >= 2:\n",
    "            torch.nn.init.xavier_uniform_(param[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(logits: torch.tensor, targets: torch.tensor):\n",
    "    return nn.CrossEntropyLoss(ignore_index=0)(logits.view(-1, len(vocab_trg)), targets.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    # train 모드로 변경\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    \n",
    "    for step, batch in bar:\n",
    "        # 데이터 로더가 딕셔너리를 반환하는 경우\n",
    "        if isinstance(batch, dict):\n",
    "            src = batch['src'].to(device)\n",
    "            trg_input = batch['trg_input'].to(device) \n",
    "            trg_output = batch['trg_output'].to(device)\n",
    "        # 데이터 로더가 튜플이나 리스트를 반환하는 경우\n",
    "        elif isinstance(batch, (tuple, list)):\n",
    "            src, trg_input, trg_output = batch\n",
    "            src = src.to(device)\n",
    "            trg_input = trg_input.to(device)\n",
    "            trg_output = trg_output.to(device)\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 순전파\n",
    "        logits, output = model(enc_src=src, dec_src=trg_input)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(logits, trg_output)\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 그래디언트 클리핑\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 스케줄러 업데이트\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 손실 및 정확도 업데이트\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_accuracy = np.mean(\n",
    "            output.view(-1).detach().cpu().numpy() == trg_output.view(-1).detach().cpu().numpy())\n",
    "        \n",
    "        accuracy += running_accuracy\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        # 현재 에포크의 평균 손실 계산\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        # 진행 바 업데이트\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, \n",
    "            Train_Loss=epoch_loss, \n",
    "            LR=optimizer.param_groups[0][\"lr\"], \n",
    "            Accuracy=accuracy / np.float64(step+1)\n",
    "        )\n",
    "    \n",
    "    # 전체 에포크의 평균 정확도 계산\n",
    "    accuracy /= len(dataloader)\n",
    "    \n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    \n",
    "    # 손실과 정확도 반환\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, batch in bar:\n",
    "        # 데이터 로더가 딕셔너리를 반환하는 경우\n",
    "        if isinstance(batch, dict):\n",
    "            src = batch['src'].to(device)\n",
    "            trg_input = batch['trg_input'].to(device) \n",
    "            trg_output = batch['trg_output'].to(device)\n",
    "        # 데이터 로더가 튜플이나 리스트를 반환하는 경우\n",
    "        elif isinstance(batch, (tuple, list)):\n",
    "            src, trg_input, trg_output = batch\n",
    "            src = src.to(device)\n",
    "            trg_input = trg_input.to(device)\n",
    "            trg_output = trg_output.to(device)\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        # MPS는 일부 연산에서 문제가 발생할 수 있으므로, 필요 시 CPU로 폴백\n",
    "        try:\n",
    "            logits, output = model(enc_src=src, dec_src=trg_input)\n",
    "            loss = criterion(logits, trg_output)\n",
    "        except RuntimeError as e:\n",
    "            # MPS에서 지원하지 않는 연산이 있을 경우\n",
    "            if \"MPS\" in str(e):\n",
    "                print(f\"MPS 오류 발생, CPU로 폴백: {e}\")\n",
    "                # 데이터와 모델을 CPU로 임시 이동\n",
    "                model_cpu = model.to(\"cpu\")\n",
    "                src_cpu = src.to(\"cpu\")\n",
    "                trg_input_cpu = trg_input.to(\"cpu\")\n",
    "                trg_output_cpu = trg_output.to(\"cpu\")\n",
    "                \n",
    "                logits, output = model_cpu(enc_src=src_cpu, dec_src=trg_input_cpu)\n",
    "                loss = criterion(logits, trg_output_cpu)\n",
    "                \n",
    "                # 모델을 다시 원래 장치로 복원\n",
    "                model.to(device)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        # 실시간으로 정보를 표시하기 위한 epoch loss\n",
    "        val_loss = running_loss / dataset_size\n",
    "        \n",
    "        # 출력을 CPU로 이동하여 NumPy 배열로 변환\n",
    "        output_cpu = output.view(-1).detach().cpu().numpy()\n",
    "        trg_output_cpu = trg_output.view(-1).detach().cpu().numpy()\n",
    "        running_accuracy = np.mean(output_cpu == trg_output_cpu)\n",
    "        \n",
    "        accuracy += running_accuracy\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[0][\"lr\"], \n",
    "            accuracy=accuracy / float(step + 1)  # np.float64 대신 float 사용\n",
    "        )\n",
    "\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_step=10,\n",
    "):\n",
    "    # To automatically log graidents\n",
    "    # wandb.watch(model, log_freq=100)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"[INFO] Using Apple Silicon GPU with MPS\")\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\".format(torch.cuda.get_device_name()))\n",
    "    else:\n",
    "        print(\"[INFO] Using CPU\")\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # num_epochs만큼, train과 val을 실행한다\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "\n",
    "        train_epoch_loss, train_accuracy = train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            dataloader=train_dataloader,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = valid_one_epoch(\n",
    "            model, valid_dataloader, device=device, epoch=epoch\n",
    "        )\n",
    "\n",
    "        history[f\"{metric_prefix}Train Loss\"].append(train_epoch_loss)\n",
    "        history[f\"{metric_prefix}Train Accuracy\"].append(train_accuracy)\n",
    "        history[f\"{metric_prefix}Valid Loss\"].append(val_loss)\n",
    "        history[f\"{metric_prefix}Valid Accuracy\"].append(val_accuracy)\n",
    "\n",
    "        # Log the metrics\n",
    "        # wandb.log(\n",
    "        #     {\n",
    "        #         f\"{metric_prefix}Train Loss\": train_epoch_loss,\n",
    "        #         f\"{metric_prefix}Valid Loss\": val_loss,\n",
    "        #         f\"{metric_prefix}Train Accuracy\": train_accuracy,\n",
    "        #         f\"{metric_prefix}Valid Accuracy\": val_accuracy,\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        print(f\"Valid Loss: {val_loss}\")\n",
    "\n",
    "        # deep copy the model\n",
    "        if val_loss <= best_loss:\n",
    "            early_stop_counter = 0\n",
    "\n",
    "            print(\n",
    "                f\"Validation Loss improved ({best_loss} ---> {val_loss})\"\n",
    "            )\n",
    "\n",
    "            # Update Best Loss\n",
    "            best_loss = val_loss\n",
    "            \n",
    "            # MacOS에서는 모델 저장 전에 CPU로 이동 (권장)\n",
    "            if 'mps' in str(device):\n",
    "                cpu_model = copy.deepcopy(model).to('cpu')\n",
    "                best_model_wts = copy.deepcopy(cpu_model.state_dict())\n",
    "                \n",
    "                PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(file_prefix, epoch, best_loss)\n",
    "                torch.save(cpu_model.state_dict(), PATH)\n",
    "                torch.save(cpu_model.state_dict(), f\"{file_prefix}best_{epoch}epoch.bin\")\n",
    "            else:\n",
    "                # 기존 방식대로 저장\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(file_prefix, epoch, best_loss)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                torch.save(model.state_dict(), f\"{file_prefix}best_{epoch}epoch.bin\")\n",
    "            \n",
    "            # Save a model file from the current directory\n",
    "            # wandb.save(PATH)\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "        elif early_stopping:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter > early_stopping_step:\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(\n",
    "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 3600,\n",
    "            (time_elapsed % 3600) // 60,\n",
    "            (time_elapsed % 3600) % 60,\n",
    "        )\n",
    "    )\n",
    "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Apple Silicon GPU with MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:48<00:00,  8.44it/s, Accuracy=0.0269, Epoch=1, LR=4.38e-5, Train_Loss=7.7] \n",
      "100%|██████████| 32/32 [00:02<00:00, 13.52it/s, Epoch=1, LR=4.38e-5, Valid_Loss=6.09, accuracy=0.045] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.090151660919189\n",
      "Validation Loss improved (inf ---> 6.090151660919189)\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [06:08<00:00,  7.98it/s, Accuracy=0.0673, Epoch=2, LR=1.56e-5, Train_Loss=6.56]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.71it/s, Epoch=2, LR=1.56e-5, Valid_Loss=6.2, accuracy=0.072]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.202142143249512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:49<00:00,  8.43it/s, Accuracy=0.0878, Epoch=3, LR=8.58e-5, Train_Loss=6.14]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.80it/s, Epoch=3, LR=8.58e-5, Valid_Loss=6.46, accuracy=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.45847811126709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:52<00:00,  8.35it/s, Accuracy=0.0989, Epoch=4, LR=7.91e-5, Train_Loss=5.89]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.53it/s, Epoch=4, LR=7.91e-5, Valid_Loss=6.6, accuracy=0.0661] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.604831245422363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:45<00:00,  8.51it/s, Accuracy=0.106, Epoch=5, LR=1.22e-5, Train_Loss=5.71]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.79it/s, Epoch=5, LR=1.22e-5, Valid_Loss=6.77, accuracy=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.76709508895874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2942/2942 [05:47<00:00,  8.46it/s, Accuracy=0.11, Epoch=6, LR=5.22e-5, Train_Loss=5.57] \n",
      "100%|██████████| 32/32 [00:01<00:00, 20.25it/s, Epoch=6, LR=5.22e-5, Valid_Loss=6.86, accuracy=0.0605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 6.857190994262695\n",
      "Training complete in 0h 35m 28s\n",
      "Best Loss: 6.0902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Transformer(\n",
       "   (encoder): Encoder(\n",
       "     (embedding): Embedding(35629, 64, padding_idx=0)\n",
       "     (positionalEncoding): PositionalEncodeing()\n",
       "     (encoder_layers): ModuleList(\n",
       "       (0-1): 2 x EncoderLayer(\n",
       "         (multiHeadAttention): MultiHeadAttention(\n",
       "           (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (layerNorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "         (ffn): FeedForwardLayer(\n",
       "           (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "           (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (layerNorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (embedding): Embedding(39411, 64)\n",
       "     (positionalEmbedding): PositionalEncodeing()\n",
       "     (decoder_layers): ModuleList(\n",
       "       (0-1): 2 x DecoderLayers(\n",
       "         (multiHeadAttention1): MultiHeadAttention(\n",
       "           (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (LayerNorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "         (multiHeadAttention2): MultiHeadAttention(\n",
       "           (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (LayerNorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "         (ffn): FeedForwardLayer(\n",
       "           (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "           (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (LayerNorm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "         (dropout3): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (finalFc): Linear(in_features=64, out_features=39411, bias=True)\n",
       "   )\n",
       "   (output_layer): Linear(in_features=64, out_features=1000, bias=True)\n",
       " ),\n",
       " defaultdict(list,\n",
       "             {'Train Loss': [7.695121183223442,\n",
       "               6.560533275107567,\n",
       "               6.13659202172158,\n",
       "               5.885811904301401,\n",
       "               5.709215138778045,\n",
       "               5.573963301946496],\n",
       "              'Train Accuracy': [np.float64(0.0269325486318691),\n",
       "               np.float64(0.06730449483759388),\n",
       "               np.float64(0.08784718795619838),\n",
       "               np.float64(0.09891749350966483),\n",
       "               np.float64(0.10562245856785464),\n",
       "               np.float64(0.10985076681989366)],\n",
       "              'Valid Loss': [6.090151660919189,\n",
       "               6.202142143249512,\n",
       "               6.45847811126709,\n",
       "               6.604831245422363,\n",
       "               6.76709508895874,\n",
       "               6.857190994262695],\n",
       "              'Valid Accuracy': [np.float64(0.04498796296571379),\n",
       "               np.float64(0.0720194424925155),\n",
       "               np.float64(0.0678183808713982),\n",
       "               np.float64(0.0660563630875699),\n",
       "               np.float64(0.06258817479526886),\n",
       "               np.float64(0.060461833561793785)]}))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb 초기화\n",
    "# wandb.init(project=\"korean-english-translator\", name=\"transformer-training\")\n",
    "\n",
    "# 학습 실행\n",
    "run_training(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=100, eta_min=1e-5),\n",
    "    device=device,\n",
    "    num_epochs=6,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_step=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final.bin')\n",
    "# wandb.save('final.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cpu (MPS 호환성 문제로 CPU 사용)\n",
      "어휘 사전 사용 준비 완료\n",
      "모델 로드 완료: final.bin\n",
      "\n",
      "=== 테스트 번역 ===\n",
      "\n",
      "한국어: 안녕하세요. 만나서 반갑습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/qqwf31rd1sx1kmrxvwshn7gr0000gn/T/ipykernel_89054/1914359106.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_obj = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어: The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "한국어: 오늘 날씨가 정말 좋네요.\n",
      "영어: The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "한국어: 인공지능 번역기술은 매우 흥미롭습니다.\n",
      "영어: The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "한국어: 저는 한국어를 공부하고 있습니다.\n",
      "영어: The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "한국어: 이 모델은 한국어를 영어로 번역합니다.\n",
      "영어: The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "=== 대화형 번역 (종료: q) ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m영어 번역: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 130\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 대화형 번역 (종료: q) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m한국어 문장 입력: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformer-ko2en/lib/python3.9/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformer-ko2en/lib/python3.9/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 필요한 상수 정의\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "# MPS 사용 가능하면 사용\n",
    "device = torch.device('cpu')\n",
    "print(f\"사용 장치: {device} (MPS 호환성 문제로 CPU 사용)\")\n",
    "\n",
    "\n",
    "# 이미 생성된 어휘 사전 사용\n",
    "# vocab_src, vocab_trg는 이미 정의되어 있다고 가정\n",
    "print(\"어휘 사전 사용 준비 완료\")\n",
    "\n",
    "# 모델 클래스 정의가 필요합니다. (이미 정의되어 있다고 가정)\n",
    "# 실제 환경에 맞게 import 구문 수정 필요\n",
    "# Transformer 클래스는 이미 정의되어 있다고 가정\n",
    "\n",
    "# 모델 초기화 (학습 시 사용한 동일한 파라미터 필요)\n",
    "\n",
    "# 모델 생성\n",
    "model = Transformer(\n",
    "    N=N, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    num_head=NUM_HEAD, \n",
    "    inner_dim=INNER_DIM, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "model_path = 'final.bin'  # 저장된 모델 경로\n",
    "try:\n",
    "    # 로드된 객체 확인\n",
    "    loaded_obj = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # 객체 유형에 따라 처리\n",
    "    if isinstance(loaded_obj, dict) and 'state_dict' in loaded_obj:\n",
    "        model.load_state_dict(loaded_obj['state_dict'])\n",
    "    elif isinstance(loaded_obj, dict):\n",
    "        model.load_state_dict(loaded_obj)\n",
    "    else:\n",
    "        model = loaded_obj\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()  # 평가 모드 설정\n",
    "    print(f\"모델 로드 완료: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"모델 로드 오류: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Mecab 초기화\n",
    "mecab = Mecab()\n",
    "\n",
    "# 번역 함수\n",
    "def translate(sentence, max_length=50):\n",
    "    \"\"\"\n",
    "    한국어 문장을 영어로 번역\n",
    "    \"\"\"\n",
    "    # 형태소 분석\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    \n",
    "    # 특수 토큰 추가\n",
    "    tokens = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "    \n",
    "    # 토큰을 인덱스로 변환\n",
    "    src_indices = [vocab_src.get(token, vocab_src[UNK_TOKEN]) for token in tokens]\n",
    "    \n",
    "    # 텐서 변환\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 번역 수행\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(src_tensor)\n",
    "        \n",
    "        # 디코더 입력 초기화 (시작 토큰)\n",
    "        trg_indices = [vocab_trg[SOS_TOKEN]]\n",
    "        trg_tensor = torch.LongTensor([trg_indices]).to(device)\n",
    "        \n",
    "        # 토큰 생성\n",
    "        for _ in range(max_length):\n",
    "            # 다음 토큰 예측\n",
    "            logits, output = model(src_tensor, trg_tensor)\n",
    "            \n",
    "            # 가장 확률 높은 토큰 선택\n",
    "            pred_token = output[0, -1].item()\n",
    "            \n",
    "            # 예측 토큰 추가\n",
    "            trg_indices.append(pred_token)\n",
    "            trg_tensor = torch.LongTensor([trg_indices]).to(device)\n",
    "            \n",
    "            # 종료 토큰이 나오면 중단\n",
    "            if pred_token == vocab_trg[EOS_TOKEN]:\n",
    "                break\n",
    "    \n",
    "    # 토큰을 단어로 변환\n",
    "    translated_tokens = [idx2word_trg.get(idx, UNK_TOKEN) for idx in trg_indices]\n",
    "    \n",
    "    # 시작 및 종료 토큰 제외\n",
    "    if translated_tokens[-1] == EOS_TOKEN:\n",
    "        translated_tokens = translated_tokens[1:-1]\n",
    "    else:\n",
    "        translated_tokens = translated_tokens[1:]\n",
    "    \n",
    "    # 결과 문장 반환\n",
    "    return ' '.join(translated_tokens)\n",
    "\n",
    "# 테스트 및 대화형 인터페이스\n",
    "def main():\n",
    "    # 테스트 문장\n",
    "    test_sentences = [\n",
    "        \"안녕하세요. 만나서 반갑습니다.\",\n",
    "        \"오늘 날씨가 정말 좋네요.\",\n",
    "        \"인공지능 번역기술은 매우 흥미롭습니다.\",\n",
    "        \"저는 한국어를 공부하고 있습니다.\",\n",
    "        \"이 모델은 한국어를 영어로 번역합니다.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== 테스트 번역 ===\")\n",
    "    for sentence in test_sentences:\n",
    "        print(f\"\\n한국어: {sentence}\")\n",
    "        translated = translate(sentence)\n",
    "        print(f\"영어: {translated}\")\n",
    "    \n",
    "    print(\"\\n=== 대화형 번역 (종료: q) ===\")\n",
    "    while True:\n",
    "        user_input = input(\"\\n한국어 문장 입력: \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "        \n",
    "        translated = translate(user_input)\n",
    "        print(f\"영어 번역: {translated}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-ko2en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
