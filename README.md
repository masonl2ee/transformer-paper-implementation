# ðŸ”Ž Transformer: Attention Is All You Need

This repository contains an implementation of the Transformer architecture as proposed in the paper:  
**[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** by Vaswani et al.

The Transformer is a fully attention-based architecture that removes recurrence entirely and achieves state-of-the-art performance on sequence transduction tasks such as machine translation.

---

## ðŸ“„ Paper

> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017).  
> **Attention is all you need.** *Advances in neural information processing systems*, 30.  
> ðŸ“Ž [NIPS Link](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

---

## ðŸ§  Key Features

- Encoder-Decoder Transformer architecture
- Multi-head self-attention
- Positional encoding
- Masking for auto-regressive decoding
- Training with cross-entropy loss
- Minimal, readable implementation for educational purposes

---

## ðŸš€ Getting Started

1. Clone the repository:
```bash
git clone https://github.com/your-username/transformer-paper-implementation.git
cd transformer-paper-implementation
```

2. install dependensies
```bash
pip install -r requirements.txt
```